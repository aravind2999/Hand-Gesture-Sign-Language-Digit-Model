{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2062 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "image_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "image_generator = image_datagen.flow_from_directory('C:\\\\Users\\\\ARAVIND REDDY\\\\Downloads\\\\Sign_Language_digits', target_size = (100,100), class_mode = 'categorical', batch_size = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 2.4584 - categorical_accuracy: 0.1500\n",
      "Epoch 2/35\n",
      "8/8 [==============================] - 1s 144ms/step - loss: 2.3219 - categorical_accuracy: 0.0750\n",
      "Epoch 3/35\n",
      "8/8 [==============================] - 1s 141ms/step - loss: 2.2987 - categorical_accuracy: 0.1125\n",
      "Epoch 4/35\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 2.3023 - categorical_accuracy: 0.1125\n",
      "Epoch 5/35\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 2.2640 - categorical_accuracy: 0.1625\n",
      "Epoch 6/35\n",
      "8/8 [==============================] - 1s 146ms/step - loss: 2.2270 - categorical_accuracy: 0.2875\n",
      "Epoch 7/35\n",
      "8/8 [==============================] - 1s 146ms/step - loss: 2.0361 - categorical_accuracy: 0.2875\n",
      "Epoch 8/35\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 1.8475 - categorical_accuracy: 0.4000\n",
      "Epoch 9/35\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 1.5362 - categorical_accuracy: 0.3875\n",
      "Epoch 10/35\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.3673 - categorical_accuracy: 0.5139\n",
      "Epoch 11/35\n",
      "8/8 [==============================] - 1s 133ms/step - loss: 1.4583 - categorical_accuracy: 0.4875\n",
      "Epoch 12/35\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 1.0802 - categorical_accuracy: 0.6000\n",
      "Epoch 13/35\n",
      "8/8 [==============================] - 1s 149ms/step - loss: 1.2554 - categorical_accuracy: 0.5500\n",
      "Epoch 14/35\n",
      "8/8 [==============================] - 1s 135ms/step - loss: 0.8914 - categorical_accuracy: 0.7375\n",
      "Epoch 15/35\n",
      "8/8 [==============================] - 1s 145ms/step - loss: 0.8239 - categorical_accuracy: 0.6625\n",
      "Epoch 16/35\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 1.0176 - categorical_accuracy: 0.6375\n",
      "Epoch 17/35\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 1.0514 - categorical_accuracy: 0.6000\n",
      "Epoch 18/35\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 0.7477 - categorical_accuracy: 0.7625\n",
      "Epoch 19/35\n",
      "8/8 [==============================] - 1s 135ms/step - loss: 0.8636 - categorical_accuracy: 0.7125\n",
      "Epoch 20/35\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 0.6178 - categorical_accuracy: 0.7875\n",
      "Epoch 21/35\n",
      "8/8 [==============================] - 1s 133ms/step - loss: 0.6124 - categorical_accuracy: 0.8000\n",
      "Epoch 22/35\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 0.6808 - categorical_accuracy: 0.7500\n",
      "Epoch 23/35\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 0.7250 - categorical_accuracy: 0.7375\n",
      "Epoch 24/35\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 0.5110 - categorical_accuracy: 0.8250\n",
      "Epoch 25/35\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 0.7627 - categorical_accuracy: 0.7750\n",
      "Epoch 26/35\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 0.5984 - categorical_accuracy: 0.7625\n",
      "Epoch 27/35\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 0.6296 - categorical_accuracy: 0.8000\n",
      "Epoch 28/35\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 0.5508 - categorical_accuracy: 0.8125\n",
      "Epoch 29/35\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 0.6092 - categorical_accuracy: 0.7625\n",
      "Epoch 30/35\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 0.5302 - categorical_accuracy: 0.8125\n",
      "Epoch 31/35\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 0.4511 - categorical_accuracy: 0.8750\n",
      "Epoch 32/35\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 0.4079 - categorical_accuracy: 0.8500\n",
      "Epoch 33/35\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 0.4897 - categorical_accuracy: 0.8500\n",
      "Epoch 34/35\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.5232 - categorical_accuracy: 0.8250\n",
      "Epoch 35/35\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.5052 - categorical_accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x203b176a788>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "ann = tf.keras.models.Sequential()\n",
    "ann.add(tf.keras.layers.Conv2D(64,(3,3),input_shape = (100,100,3), activation = 'relu'))\n",
    "ann.add(tf.keras.layers.MaxPooling2D(2,2))\n",
    "ann.add(tf.keras.layers.Conv2D(64 ,(3,3), activation = 'relu'))\n",
    "ann.add(tf.keras.layers.MaxPooling2D(2,2))\n",
    "ann.add(tf.keras.layers.Conv2D(64,(3,3), activation = 'relu'))\n",
    "ann.add(tf.keras.layers.MaxPooling2D(2,2))\n",
    "ann.add(tf.keras.layers.Flatten()) \n",
    "ann.add(tf.keras.layers.Dense(512, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(10, activation = 'softmax'))\n",
    "ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "ann.fit_generator(image_generator, steps_per_epoch = 8, epochs = 35, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-72626e253167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msuccessive_outputs\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvisualization_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuccessive_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msuccessive_feature_maps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualization_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# These are the names of the layers, so can have them as part of our plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m     87\u001b[0m           method.__name__))\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m             \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m             \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0marguments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mincorrect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \"\"\"\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m       raise ValueError(\n\u001b[0;32m    577\u001b[0m           \u001b[1;34m\"Arguments and signature arguments do not match. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "successive_outputs =  [layer.output for layer in ann.layers[:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = ann.input, outputs = successive_outputs)\n",
    "successive_feature_maps = visualization_model.predict(image_generator)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in ann.layers[1:]]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    if len(feature_map.shape) == 3:\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[2]\n",
    "    # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "      # Postprocess the feature to make it visually palatable\n",
    "            x = feature_map[0, :, :, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "      # We'll tile each filter into this big horizontal grid\n",
    "            display_grid[:, i * size : (i + 1) * size] = x\n",
    "    # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:\\\\Users\\\\ARAVIND REDDY\\\\Downloads\\\\Sign_Language_digits\\\\5\\\\IMG_1143.JPG',target_size = (100,100))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = ann.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background captured\n",
      "pred_array: [[9.5280373e-01 2.7120754e-02 1.8360272e-02 2.9399976e-05 8.9372888e-06\n",
      "  2.0894108e-08 4.7134017e-04 8.4728433e-04 3.5809929e-04 4.5366324e-08]]\n",
      "Result: 0\n",
      "0.95280373\n",
      "0\n",
      "pred_array: [[1.8003027e-01 5.7727320e-04 1.2417813e-02 6.6567075e-01 5.3677084e-03\n",
      "  4.3641306e-03 4.6985883e-02 7.1195170e-04 6.6998564e-02 1.6875586e-02]]\n",
      "Result: 3\n",
      "0.66567075\n",
      "3\n",
      "pred_array: [[5.3277858e-02 8.3297353e-05 3.8334776e-02 7.5741124e-01 5.5069742e-03\n",
      "  3.5546136e-03 4.2544331e-02 2.3266625e-04 9.4356686e-02 4.6975347e-03]]\n",
      "Result: 3\n",
      "0.75741124\n",
      "3\n",
      "pred_array: [[1.21437013e-01 1.20963756e-04 6.11360595e-02 6.83635056e-01\n",
      "  3.99249746e-03 2.52880948e-03 5.07696234e-02 2.80419335e-04\n",
      "  7.27069154e-02 3.39274411e-03]]\n",
      "Result: 3\n",
      "0.68363506\n",
      "3\n",
      "pred_array: [[5.6687689e-01 6.1035564e-04 7.3068701e-02 3.0396545e-01 1.0054929e-03\n",
      "  3.9896808e-04 2.3421708e-02 6.1448367e-04 2.9294180e-02 7.4376323e-04]]\n",
      "Result: 0\n",
      "0.5668769\n",
      "0\n",
      "pred_array: [[6.3044415e-03 2.2289911e-05 1.2472922e-05 9.8951322e-01 1.8233295e-05\n",
      "  7.1609509e-04 1.6371162e-03 1.1218657e-05 1.6827306e-03 8.2049635e-05]]\n",
      "Result: 3\n",
      "0.9895132\n",
      "3\n",
      "pred_array: [[1.20506555e-01 7.43629992e-01 1.22601964e-01 4.77723870e-03\n",
      "  1.28716696e-04 7.65011555e-06 4.08809679e-03 2.95046996e-03\n",
      "  1.21764361e-03 9.17752550e-05]]\n",
      "Result: 1\n",
      "0.74363\n",
      "1\n",
      "pred_array: [[1.4626026e-01 7.4263901e-01 1.0150654e-01 2.9908975e-03 7.1750124e-05\n",
      "  4.8142301e-06 3.6418249e-03 2.0781504e-03 7.4700138e-04 5.9867427e-05]]\n",
      "Result: 1\n",
      "0.742639\n",
      "1\n",
      "pred_array: [[1.6300187e-02 2.1772928e-05 2.1720543e-03 3.7538826e-01 1.6195488e-01\n",
      "  1.7465121e-01 1.7653124e-01 1.2130659e-02 7.9910740e-02 9.3905831e-04]]\n",
      "Result: 3\n",
      "0.37538826\n",
      "3\n",
      "pred_array: [[2.3677580e-02 7.1664464e-05 2.6502987e-02 6.6441190e-01 3.4099654e-03\n",
      "  4.1812835e-03 2.7647793e-01 2.5540558e-04 7.0260686e-04 3.0863311e-04]]\n",
      "Result: 3\n",
      "0.6644119\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-064cf9ac2453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mcamera\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     cv2.rectangle(frame, (int(cap_region_x_begin * frame.shape[1]), 0),\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "cap_region_x_begin = 0.5  # start point/total width\n",
    "cap_region_y_end = 0.8  \n",
    "threshold = 60\n",
    "isBgCaptured = 0\n",
    "blur_value = 41\n",
    "  # GaussianBlur parameter\n",
    "bgSubThreshold = 50\n",
    "learningRate = 0\n",
    "camera = cv2.VideoCapture(0)\n",
    "def predict(img):\n",
    "    img = np.array(img, dtype = 'float32')\n",
    "    img /=255\n",
    "    pred_array = ann.predict(img)\n",
    "    print(f'pred_array: {pred_array}')\n",
    "    result = numbers[np.argmax(pred_array)]\n",
    "    print(f'Result: {result}')\n",
    "    print(max(pred_array[0]))\n",
    "    score = float(\"%0.2f\" % (max(pred_array[0]) * 100))\n",
    "    print(result)\n",
    "    return result, score\n",
    "def remove_background(frame):\n",
    "    fgmask = bgModel.apply(frame,learningRate=learningRate)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    fgmask = cv2.erode(fgmask, kernel, iterations=1)\n",
    "    res = cv2.bitwise_and(frame, frame, mask=fgmask)\n",
    "    return res\n",
    "    \n",
    "while camera.isOpened():\n",
    "    ret, frame = camera.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    cv2.rectangle(frame, (int(cap_region_x_begin * frame.shape[1]), 0),\n",
    "                  (frame.shape[1], int(cap_region_y_end * frame.shape[0])), (255, 0, 0), 2)\n",
    "    k = cv2.waitKey(10)\n",
    "    cv2.imshow('Window', frame)\n",
    "    if isBgCaptured == 1:\n",
    "        img = remove_background(frame)\n",
    "        img = img[0:int(cap_region_y_end * frame.shape[0]),\n",
    "                   int(cap_region_x_begin * frame.shape[1]):frame.shape[1]]\n",
    "        cv2.imshow('mask',img)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (blur_value, blur_value), 0)\n",
    "        ret, thresh = cv2.threshold(blur, threshold, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        cv2.imshow('ori', thresh)\n",
    "        thresh1 = copy.deepcopy(thresh)\n",
    "        _, contours, hierarchy = cv2.findContours(thresh1,  cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        length = len(contours)\n",
    "        maxArea = -1\n",
    "        if length>0:\n",
    "            for i in range(length):\n",
    "                temp = contours[i]\n",
    "                area = cv2.contourArea(temp)\n",
    "                if area > maxArea:\n",
    "                    maxArea = area\n",
    "                    ci = i\n",
    "            res = contours[ci]\n",
    "            hull = cv2.convexHull(res)\n",
    "            drawing = np.zeros(img.shape, np.uint8)\n",
    "            cv2.drawContours(drawing, [res], 0, (0, 255, 0), 2)\n",
    "            cv2.drawContours(drawing, [hull], 0, (0, 0, 255), 3)\n",
    "        cv2.imshow('output', drawing)\n",
    "    \n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('b'):\n",
    "        bgModel = cv2.createBackgroundSubtractorMOG2(0, bgSubThreshold)\n",
    "        time.sleep(2)\n",
    "        isBgCaptured = 1\n",
    "        print('Background captured')\n",
    "    elif k == ord('r'): \n",
    "        time.sleep(1)\n",
    "        bgModel = None\n",
    "        triggerSwitch  = False\n",
    "        isBgCaptured = 0\n",
    "        print(\"Reset Background\")\n",
    "    elif k == 32:\n",
    "        cv2.imshow('original', frame)\n",
    "        target = np.stack((thresh,) * 3, axis=-1)\n",
    "        target = cv2.resize(target, (100, 100))\n",
    "        target = target.reshape(1, 100, 100, 3)\n",
    "        prediction, score = predict(target)\n",
    "        camera.release\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
